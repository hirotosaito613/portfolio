<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Hiroto Saito</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v5.15.3/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Alegreya+Sans+SC:300" rel="stylesheet">
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Maven+Pro&family=Molengo&family=PT+Sans&family=Roboto:wght@100&display=swap" rel="stylesheet">

        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
        <link rel="stylesheet" href="css/remodal.css">
        <link rel="stylesheet" href="css/remodal-default-theme.css">

        <!-- Core theme JS-->
        <script src="js/jquery-3.6.0.min.js"></script>
        <script src="js/remodal.min.js"></script>
        <script src="js/scripts.js"></script>

        <!-- Bootstrap core JS-->
        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.0/dist/js/bootstrap.bundle.min.js"></script>
        
        

    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <!-- <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">齊藤 寛人 / Hiroto Saito</span>
                   <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="..." /></span>
            </a>-->
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button> 
            <h2 class="navbar-text">
            Hiroto Saito
            </h2>
            <hr class="mb-5" />
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#home">Home</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#works">Works</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#education">Education</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#experience">Experience</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#publications">Publications</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#awards">Awards</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#contact">Contact</a></li>
                    
                </ul>
            </div>
        </nav>

        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="home">
                <div class="resume-section-content">
                    <h3 class="mb-6">
                        齊藤 寛人 / Hiroto Saito
                    </h3>
                    <p>
                    <img src="assets/img/profile.png" class="profile" />
                        </p>
                    <p class="lead mb-3">
                        Dr. Hiroto Saito is a project assistant professor in the Research Center for Advanced Science and Technology at the University of Tokyo, Japan.
                        He received my Ph.D. in Frontier Media Science from Graduate School of Advanced Mathematical Sciences, Meiji University, Japan. 
                        His research interests include human motor perception and learning, cognitive science about self-attribution and embodiment, and the design of human-computer interaction.
                    </p>

                    <p class="lead mb-5">
                        2019年明治大学大学院先端数理科学研究科博士後期課程修了．
                        2019年東京大学先端科学技術研究センター特任研究員，2022年東京大学先端科学技術研究センター特任助教，現在に至る．
                        運動学習・運動知覚およびそれらの特性を利用したヒューマンコンピュータインタラクションの設計に関する研究に従事．
                        博士（理学）．
                        ACM・情報処理学会・VR学会・認知科学会・モーターコントロール研究会各会員．
                    </p>
                
            
                 
                    <!-- Works-->
                    <br  id = "works"/>
            
            
                    <h3 class="mb-5">Works</h3>

                    <div class="works">

                         <!--  ------------ transparency ------------ -->
                         <div class="img-thumbnail">
                            <a target="_blank" rel="noopener noreferrer"  class="social-icon" data-remodal-target="transparency">
                                <p class="works"><img class="works" src="assets/img/transparency-square-min.jpg" alt="transparency"></p> 
                            </a>
                            <div class="remodal" data-remodal-id="transparency" data-remodal-options="hashTracking:false">
                                <button data-remodal-action="close" class="remodal-close"></button>
                                <img src="assets/img/transparency.jpg" class="works-modal" alt="transparency" />
                                
                                <h3 class="mb-5">Transparency in Human-Machine Mutual Action</h3>
                                
                                <h4 class="mb-1">Abstract</h4>
                                <p class="works-modal-text">
                                    Recent advances in human-computer integration (HInt) have focused on the development of human-machine systems, where both human and machine autonomously act upon each other. 
                                    However, a key challenge in designing such systems is augmenting the user’s physical abilities while maintaining their sense of self-attribution. 
                                    This challenge is particularly prevalent when both human and machine are capable of acting upon each other, thereby creating a human-machine mutual action (HMMA) system. 
                                    To address this challenge, we present a design framework that is based on the concept of transparency. 
                                    We define transparency in HInt as the degree to which users can self-attribute an experience when machines intervene in the users’ action. 
                                    Using this framework, we form a set of design guidelines and an approach for designing HMMA systems. 
                                    By using transparency as our focus, we aim to provide a design approach for not only achieving human-machine fusion into a single agent, but also controlling the degrees of fusion at will. 
                                    This study also highlights the effectiveness of our design approach through an analysis of existing studies that developed HMMA systems. 
                                    Further development of our design approach is discussed, and future prospects for HInt and HMMA system designs are presented.                                
                                </p>

                                <hr class="m-5" />
                                <h4 class="mb-1">Publications</h4>
                                <p class="works-modal-text">
                                  Hiroto Saito, Arata Horie, Azumi Maekawa, Seito Matsubara, Sohei Wakisaka, Zendai Kashino, Shunichi Kasahara, and Masahiko Inami. 2021. Transparency in Human-Machine Mutual Action.  In <em> Journal of Robotics and Mechatronics</em>, Vol.33, No.5, pp. 987-1003. DOI:<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.20965/jrm.2021.p0987" rel="nofollow">https://doi.org/10.20965/jrm.2021.p0987</a>
                                </p>

                                
                                <button data-remodal-action="cancel" class="remodal-cancel">close</button>
                            </div> 
                        </div>
                   
                            <!--  ------------ hockey ------------ -->
                            <div class="img-thumbnail">
                                <a target="_blank" rel="noopener noreferrer"  class="social-icon" data-remodal-target="hockey">
                                    <p class="works"><img class="works" src="assets/img/hockey-square-min.png" alt="hockey"></p> 
                                </a>
                                <div class="remodal" data-remodal-id="hockey" data-remodal-options="hashTracking:false">
                                    <button data-remodal-action="close" class="remodal-close"></button>
                                    <img src="assets/img/hockey.png" class="works-modal" alt="hockey" />
                                    
                                    <h3 class="mb-5">Behind the Game</h3>
                                    
                                    <h4 class="mb-1">Abstract</h4>
                                    <p class="works-modal-text">
                                        When playing inter-personal sports games remotely, the time lag between user actions and feedback decreases the user’s performance and sense of agency.
                                        While computational assistance can improve performance, naive intervention independent of the context also compromises the user’s sense of agency.                                    
                                        We propose a context-aware assistance method that retrieves both user performance and sense of agency, and we demonstrate the method using air hockey (a two-dimensional physical game) as a testbed.                                    
                                        Our system includes a 2D plotter-like machine that controls the striker on half of the table surface, and a web application interface that enables manipulation of the striker from a remote location.                                    
                                        Using our system, a remote player can play against a physical opponent from anywhere through a web browser.                                    
                                        We designed the striker control assistance based on the context by computationally predicting the puck’s trajectory using a real-time captured video image.                                    
                                        With this assistance, the remote player exhibits an improved performance without compromising their sense of agency, and both players can experience the excitement of the game.                                
                                    </p>

                                    <iframe class="works-frame" width="560" height="315" src="https://www.youtube.com/embed/bBTY0O-rbZ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <hr class="m-5" />
                                    <h4 class="mb-1">Publications</h4>
                                    <p class="works-modal-text">
                                    Maekawa, Azumi, Hiroto Saito, Narin Okazaki, Shunichi Kasahara, and Masahiko Inami. 2021. Behind The Game: Implicit Spatio-Temporal Intervention in Inter-Personal Remote Physical Interactions on Playing Air Hockey.  In <em>ACM SIGGRAPH 2021 Emerging Technologies</em> (<em>SIGGRAPH &#8217;21</em>). Association for Computing Machinery, New York, NY, USA, Article 9, 1–4. DOI:<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/3450550.3465348" rel="nofollow">https://doi.org/10.1145/3450550.3465348</a>
                                    </p>
                                    <h4 class="mb-1">Link </h4>
                                    <p class="works-modal-text"><a target="_blank" rel="noopener noreferrer" href="https://s2021.siggraph.org/siggraph-2021-emerging-technologies-demonstrate-pandemic-perseverance/">SIGGRAPH 2021 EMERGING TECHNOLOGIES DEMONSTRATE PANDEMIC PERSEVERANCE</a></p>                               
                                    <p class="works-modal-text"><a target="_blank" rel="noopener noreferrer" href="https://prtimes.jp/main/html/rd/p/000000048.000042335.html">28ヵ国が参加するダイソン国際エンジニアリングアワード 2021</a></p>
                                    
                                    <button data-remodal-action="cancel" class="remodal-cancel">close</button>
                                </div> 
                            </div>
                       
                            <!--  ------------ tug ------------ -->
                            <div class="img-thumbnail">
                                <a  class="social-icon" data-remodal-target="tug">
                                    <p class="works"><img class="works" src="assets/img/tug-square-min.png" alt="tug"></p> 
                                </a>
                                <div class="remodal" data-remodal-id="tug" data-remodal-options="hashTracking:false">
                                    <button data-remodal-action="close" class="remodal-close"></button>
                                    <img src="assets/img/tug.jpg" class="works-modal" alt="tug" />
                                    
                                    <h3 class="mb-5">The Tight Game</h3>
                                    
                                    <h4 class="mb-1">Abstract</h4>
                                    <p class="works-modal-text">
                                        Physical assistance can alleviate individual differences of abilities between players to create well-balanced inter-personal physical games.
                                        However, ‘explicit’ intervention can ruin the players’ sense of agency, and cause a loss of engagements in both the player and audience.                                   
                                        We propose an implicit physical intervention system ”The Tight Game” for ‘Tug of War’ a one-dimensional physical game.  
                                        Our system includes four force sensors connected to the rope and two hidden high torque motors, which provide realtime physical assistance.                                    
                                        We designed the implicit physical assistance by leveraging human recognition of the external forces during physical actions.                                    
                                        In The Tight Game, a pair of players engage in a tug of war, and believe that they are participating in a well balanced, tight game.                                    
                                        In reality, however, an external system or person mediates the game, performing physical interventions without the players noticing.                        
                                    </p>

                                    <iframe class="works-frame" width="560" height="315" src="https://www.youtube.com/embed/LxVZ8BQmhto" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    <hr class="m-5" />
                                    <h4 class="mb-1">Publications</h4>
                                    <p class="works-modal-text">
                                    Azumi Maekawa, Shunichi Kasahara, Hiroto Saito, Daisuke Uriu, Gowrishankar Ganesh, and Masahiko Inami. 2020. The Tight Game: Implicit Force Intervention in Inter-personal Physical Interactions on Playing Tug of War. In <em>ACM SIGGRAPH 2020 Emerging Technologies</em> (<em>SIGGRAPH &#8217;20</em>). Association for Computing Machinery, New York, NY, USA, Article 10, 1–2. DOI:<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/3388534.3407301" rel="nofollow">https://doi.org/10.1145/3388534.3407301</a>
                                    </p>
                                    <h4 class="mb-1">Link </h4>
                                    <p class="works-modal-text"><a target="_blank" rel="noopener noreferrer" href="https://www.jst.go.jp/erato/inami/project_44.html">JST ERATO Inami JIZAI body project - project page</a></p>  
                                    <p class="works-modal-text"><a target="_blank" rel="noopener noreferrer" href="https://s2020.siggraph.org/siggraph-2020-to-spotlight-20-transformative-emerging-technologies/">SIGGRAPH 2020 TO SPOTLIGHT 20 TRANSFORMATIVE, EMERGING TECHNOLOGIES</a></p>                               
                                    <p class="works-modal-text"><a target="_blank" rel="noopener noreferrer" href="https://www.itmedia.co.jp/news/articles/2103/22/news022.html">綱引きの“八百長装置”　外部から引力をこっそり制御、接戦を演出　東大など「The Tight Game」開発</a></p>
                                    
                                    <button data-remodal-action="cancel" class="remodal-cancel">close</button>
                                </div>
                            </div>

                              <!--  ------------ steering ------------ -->
                              <div class="img-thumbnail">
                                <a  class="social-icon" data-remodal-target="steering">
                                    <p class="works"><img class="works" src="assets/img/steering-square-min.jpg" alt="steering"></p> 
                                </a>
                                <div class="remodal" data-remodal-id="steering" data-remodal-options="hashTracking:false">
                                    <button data-remodal-action="close" class="remodal-close"></button>   
                                    <h3 class="mb-5">Analysis of the Causes of Pseudo-haptics Using the Motion Mislearning during Steering Task</h3>
                                    
                                    <img src="assets/img/steering.jpg" alt="steering" />
                                 
                                    <h4 class="mb-1">Abstract</h4>
                                    <p class="works-modal-text">
                                        Previous studies described that Pseudo-haptics is a sensory illusion caused by the conflict between visual and haptic perception.
                                        However, this conflict could be interpreted as a result of voluntary movement.
                                        It remains unclear as to why the visuo-haptic conflict is interpreted as a passive force, not a voluntary movement.
                                        We hypothesized that the illusion of the passive force is caused by the conflict between visual feedback and motor prediction, not the haptic perception.
                                        To test this hypothesis, we performed an experiment using motor mislearning to modulate the motion of the voluntary movement that causes a conflict between motor prediction and visual feedback even if maintained congruence between visual and haptic feedback.
                                        In this experiment, subjects informed the magnitude of passive force felt subjectively during the steering task using the mouse.
                                        As a result, we confirmed the possibility that the conflict between motor prediction and visual feedback causes Pseudo-haptics.
                                    </p>

                                    <hr class="m-5" />
                                    <h4 class="mb-1">Publications</h4>
                                    <p class="works-modal-text">
                                        齊藤寛人, 福地健太郎. 運動伝染が生み出す運動予測の変調を利用したPseudo-haptics の生起要因の分析, 情報処理学会 インタラクション2019論文集 (インタラクション2019 一般講演), pp.112-121. <a target="_blank" rel="noopener noreferrer" href="http://www.interaction-ipsj.org/proceedings/2019/data/pdf/INT19013.pdf">PDF</a>
                                    </p>
                                    <!-- <h4 class="mb-1">Link </h4> -->
                                    <button data-remodal-action="cancel" class="remodal-cancel">close</button>
                                </div>
                            </div>


                             <!--  ------------ findhand ------------ -->
                             <div class="img-thumbnail">
                                <a  class="social-icon" data-remodal-target="findhand">
                                    <p class="works"><img class="works" src="assets/img/findhand-square-min.jpg" alt="findhand"></p> 
                                </a>
                                <div class="remodal" data-remodal-id="findhand" data-remodal-options="hashTracking:false">
                                    <button data-remodal-action="close" class="remodal-close"></button>
                            
                                    <h3 class="mb-5">The Effect of Predictability of Visual Motion from Motor Commands on the Recognition Process of Self-Attribution</h3>
                                    
                                    <img src="assets/img/findhand.jpg" class="works-modal" alt="findhand" />

                                    <h4 class="mb-1">Abstract</h4>
                                    <p class="works-modal-text">
                                        The recognition process of self-attribution, which is mainly caused by congruence between visual and proprioceptive information and between visual information and prediction from motor commands, has been extensively studied.
                                        However, it is still unclear as to which congruence plays the primary role in the process during the voluntary movements.
                                        We conducted a user study that distinguishes proprioceptive information and prediction from motor commands by displaying the modified images of the participants' hands in various rotation angles; this introduced the conflict between visual and proprioceptive information.
                                        The hand motions of the participants were restricted so that they could predict the visual motion of the images of their hands by the motor command even while the images were rotated.
                                        The result indicates that motion prediction plays a primary role in the recognition process of self-attribution, and this predictability depends on the motion pattern and appearance of the hand images.
                                    </p>

                                    <hr class="m-5" />
                                    <h4 class="mb-1">Publications</h4>
                                    <p class="works-modal-text">
                                        Hiroto Saito and Kentaro Fukuchi. 2018. The Effect of Predictability of Visual Motion from Motor Commands on the Recognition Process of Self-Attribution. In <i>Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems (CHI EA '18)</i>. Association for Computing Machinery, New York, NY, USA, Paper LBW124, 1–6. DOI:<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/3170427.3188512" rel="nofollow">https://doi.org/10.1145/3170427.3188512</a>
                                    </p>
                                    <p class="works-modal-text">
                                        齊藤寛人, 福地健太郎. 視覚的運動の予測可能性が自己帰属感の生起過程に及ぼす影響の調査, <i>ヒューマンインタフェース学会論文誌</i>, 2018, Vol.20, No.3, pp.301-310. DOI:<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.11184/his.20.3_301" rel="nofollow">https://doi.org/10.11184/his.20.3_301</a>
                                    </p>
                                    <!-- <h4 class="mb-1">Link </h4> -->
                                    <button data-remodal-action="cancel" class="remodal-cancel">close</button>
                                </div>
                            </div>




                        </div>
                    </div>
                </div>

            </section> 
            
            <hr class="m-5" />

            <!-- Education-->
            <section class="resume-section" id="education">
                <div class="resume-section-content">
                    <h3 class="mb-5">略歴 - Education</h3>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">明治大学大学院先端数理科学研究科博士後期課程</h5>
                            <p>先端メディアサイエンス専攻</p>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2017 - 2019</h5></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">明治大学大学院理工学研究科博士後期課程</h5>
                            <p>新領域創造専攻 ディジタルコンテンツ系</p>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2016 - 2017</h5></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">明治大学大学院理工学研究科博士前期課程</h5>
                            <p>新領域創造専攻 ディジタルコンテンツ系</p>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2014 - 2016</h5></div>
                    </div>

                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">明治大学理工学部情報科学科</h5>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2010 - 2014</h5></div>
                    </div>
                   
                   
                 
                  

                     <!-- Experience-->
                    <hr class="m-5"  id = "experience"/>
                    <h3 class="mb-5">職歴 - Experience</h3>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">東京大学先端科学技術研究センター 特任助教</h5>
                            <p>JST ERATO 稲見自在化身体プロジェクト</p>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2022 - present</h5></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">東京大学先端科学技術研究センター 特任研究員</h5>
                            <p>JST ERATO 稲見自在化身体プロジェクト</p>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2019 - 2022</h5></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">明治大学総合数理学部 助手</h5>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2017 - 2019</h5></div>
                    </div>
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-3">
                        <div class="flex-grow-1">
                            <h5 class="mb-0">明治大学理工学部 助手</h5>
                        </div>
                        <div class="flex-shrink-0"><h5 class="text-primary">2016 - 2017</h5></div>
                    </div>

                </div>
            </section>
            <hr class="m-5" />

            <!-- Publications-->
            <section class="resume-section" id="publications">
                <div class="resume-section-content">
                    <h3 class="mb-5">Publications</h3>
    
                     <h4 class="mb-3">査読付き学術論文 -  Peer Reviewed Journals</h4>
                        <ul class="mb-5">
                            <li class="mb-2">
                                Hiroto Saito, Arata Horie, Azumi Maekawa, Seito Matsubara, Sohei Wakisaka, Zendai Kashino, Shunichi Kasahara, and Masahiko Inami. 2021. Transparency in Human-Machine Mutual Action.  In <em> Journal of Robotics and Mechatronics</em>, Vol.33, No.5, pp. 987-1003. (<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.20965/jrm.2021.p0987" rel="nofollow">DOI</a>)
                            </li>  
                            <li class="mb-2">
                                堀江 新, 下林 秀輝, 齊藤 寛人, 稲見 昌彦. 回転の皮膚せん断変形に基づく分布型触覚ディスプレイの設計. <i>日本バーチャルリアリティ学会論文誌</i>, 2020, 25 巻, 4 号, p. 402-411. (<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.18974/tvrsj.25.4_402" rel="nofollow">DOI</a>)
                            </li>
                            <li class="mb-2">
                                齊藤寛人, 福地健太郎. 視覚的運動の予測可能性が自己帰属感の生起過程に及ぼす影響の調査, <i>ヒューマンインタフェース学会論文誌</i>, 2018, Vol.20, No.3, pp.301-310. (<a target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.11184/his.20.3_301" rel="nofollow">DOI</a>)
                            </li>
                            <li class="mb-2">
                                齊藤寛人, 福地健太郎. 能動的回転操作における自己帰属感の生起過程の分析, <i>日本バーチャルリアリティ学会論文誌</i>, 2017, Vol.22 No.1, pp.81-90. (<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.18974/tvrsj.22.1_81" rel="nofollow">DOI</a>)
                            </li>
                        </ul>
            
                        <h4 class="mb-3">査読付き国際会議 - Peer Reviewed International Conferences</h4>
                        <ul class="mb-5">
                            <li class="mb-2">
                                Azumi Maekawa, Hiroto Saito, Daisuke Uriu, Shunichi Kasahara, and Masahiko Inami. 2022. Machine-Mediated Teaming: Mixture of Human and Machine in Physical Gaming Experience. In <i>CHI Conference on Human Factors in Computing Systems (CHI ’22)</i>, April 29-May 5, 2022, New Orleans, LA, USA. ACM, New York, NY, USA, 11 pages. (<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/3491102.3517555" rel="nofollow">DOI</a>) <b>[co-first]</b> 
                            </li>
                            <li class="mb-2">
                                Hitoshi Kawasaki, Sohei Wakisaka, Hiroto Saito, Atsushi Hiyama, and Masahiko Inami. 2022. A System for Augmenting Humans’ ability to Learn Kendama Tricks through Virtual Reality Training. In <i>Augmented Humans 2022 (AHs2022)</i>, March 13–15, 2022, Kashiwa, Chiba, Japan. ACM, New York, NY, USA, 10 pages. (<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/3519391.3519404" rel="nofollow">DOI</a>)  
                            </li>
                            <li class="mb-2">
                                Masahiko Inami, Daisuke Uriu, Zendai Kashino, Shigeo Yoshida, Hiroto Saito, Azumi Maekawa, and Michiteru Kitazaki. 2022. Cyborgs, Human Augmentation, Cybernetics, and JIZAI Body. In <i>Augmented Humans 2022 (AHs2022)</i>, March 13–15, 2022, Kashiwa, Chiba, Japan. ACM, New York, NY, USA, 20 pages. (<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1145/3519391.3519401" rel="nofollow">DOI</a>)  
                            </li>
                            <li class="mb-2">
                                Azumi Maekawa, Hiroto Saito, Narin Okazaki, Shunichi Kasahara, and Masahiko Inami. 2021. Behind The Game: Implicit Spatio-Temporal Intervention in Inter-personal Remote Physical Interactions on Playing Air Hockey. In <i>ACM SIGGRAPH 2021 Emerging Technologies (SIGGRAPH '21)</i>. Association for Computing Machinery, New York, NY, USA, Article 9, 1–4. (<a target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.1145/3450550.3465348" rel="nofollow">DOI</a>) <b>[co-first]</b>
                            </li>
                            <li class="mb-2">
                                Azumi Maekawa, Shunichi Kasahara, Hiroto Saito, Daisuke Uriu, Gowrishankar Ganesh, and Masahiko Inami. 2020. The Tight Game: Implicit Force Intervention in Inter-personal Physical Interactions on Playing Tug of War. In <i>ACM SIGGRAPH 2020 Emerging Technologies (SIGGRAPH '20)</i>. Association for Computing Machinery, New York, NY, USA, Article 10, 1–2.(<a target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.1145/3388534.3407301" rel="nofollow">DOI</a>)
                            </li>
                            <li class="mb-2">
                                Adrien Verhulst, Eulalie Verhulst, Minori Manabe, Hiroto Saito, Shunichi Kasahara and Masahiko Inami. Investigating the Influence of Odors Visuals Representations on the Sense of Smell, a pilot study. <i>2020 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)</i>, 2020, pp. 726-727.(<a target="_blank" rel="noopener noreferrer" href="https://doi.org/10.1109/VRW50115.2020.00214" rel="nofollow">DOI</a>)
                            </li>
                            <li class="mb-2">
                                Hiroto Saito and Kentaro Fukuchi. 2018. The Effect of Predictability of Visual Motion from Motor Commands on the Recognition Process of Self-Attribution. In <i>Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems (CHI EA '18)</i>. Association for Computing Machinery, New York, NY, USA, Paper LBW124, 1–6.(<a target="_blank" rel="noopener noreferrer" href="http://dx.doi.org/10.1145/3170427.3188512" rel="nofollow">DOI</a>)
                            </li>
                            <li class="mb-2">
                                Hiroto Saito, Kentaro Fukuchi. Analysis of Identification Process of Self Image from Multiple Silhouetted Images. <i>Adjunct Proceedings of the 2014 International Conference on Advanced Visual Interfaces (AVI2014 Demo)</i>, 2014, Como, Italy, Paper No. Demo09.
                            </li>

                        </ul>
                     
                        <h4 class="mb-3">国内学会 - Domestic Conferences</h4>
                        <ul class="mb-5">
                            <li class="mb-2">
                                市橋爽介, 堀江新, 齊藤寛人, 柏野善大, 稲見昌彦. 遠赤外線による方向知覚の予備的研究. 第21回計測自動制御学会システムインテグレーション部門講演会 (SICE SI 2020), 2020年12月.
                            </li>
                            <li class="mb-2">
                                岡本直樹, 廣瀬雅治, 脇坂 崇平, 齊藤寛人, 泉原厚史, 稲見昌彦. 重心移動を用いた手部身体の変形システムの検討. 第21回計測自動制御学会システムインテグレーション部門講演会 (SICE SI 2020), 2020年12月.
                            </li>
                            <li class="mb-2">
                                村本剛毅, 齊藤寛人, 脇坂崇平, 笠原俊一, 稲見昌彦. リアルタイムとスローモーションを共存させるインタフェース. 第25回 日本バーチャルリアリティ学会大会 (VRSJ 2020), 2020年9月.
                            </li>
                            <li class="mb-2">
                                川崎仁史, 脇坂崇平, 笠原俊一, 齊藤寛人, 原口純也, 稲見昌彦. けん玉できた！VR：数分間のVRトレーニングによってけん玉の技の習得を支援するシステム. 情報処理学会　エンタテイメントコンピューティング2020(EC 2020), 2020年8月.
                            </li>
                            <li class="mb-2">
                                佐々木智也, 齊藤寛人, 脇坂崇平, 檜山敦, 稲見昌彦. 足の動作に連動するバーチャルハンドを用いた身体認知の研究. 第24回 日本バーチャルリアリティ学会大会 (VRSJ 2019), 東京, 2019年9月.
                            </li>
                            <li class="mb-2">
                                齊藤寛人, 福地健太郎. 運動伝染が生み出す運動予測の変調を利用したPseudo-haptics の生起要因の分析, 情報処理学会 インタラクション2019論文集 (インタラクション2019 一般講演), pp.112-121, 東京, 2019年3月.
                            </li>
                            <li class="mb-2">
                                齊藤寛人, 福地健太郎. 自己帰属感の生起過程におけるモーターコマンドからの予測可能性による影響の調査, 情報処理学会 インタラクション2018論文集（インタラクション2018 一般講演）, pp.25-34, 東京, 2018年3月.
                            </li>
                        </ul>

                        <h4 class="mb-3">講演</h4>
                        <ul class="mb-5">
                            <li class="mb-2">
                                映像世界と身体感覚. 招待講演, 東京経済大学, 2017年7月3日.
                            </li>
                        </ul>

                        <h4 class="mb-3">その他 - Others</h4>
                        <ul class="mb-5">
                            <li class="mb-2">    
                                齊藤寛人. 運動予測に基づく視覚的運動と自己運動との関連性の認知メカニズムについての分析. 情報処理学会学会誌「情報処理」, 2019, Vol.60 No.9, 研究会推薦博士論文速報.
                            </li>
                        </ul>
                    <hr class="m-5"  id = "awards"/>
                    <h3 class="mb-5">Awards</h3>
                    <ul>
                        <li>
                            Augmented Humans International Conference 2022 Honnorable Mentions Award
                            <p>Masahiko Inami, Daisuke Uriu, Zendai Kashino, Shigeo Yoshida, Hiroto Saito, Azumi Maekawa, and Michiteru Kitazaki. Cyborgs, Human Augmentation, Cybernetics, and JIZAI Body.</p>
                        </li>
                        <li>
                            James Dyson Award 2021 国内準優秀賞
                            <p>前川 和純，齊藤 寛人，岡﨑 菜琳，笠原 俊一．BEHIND THE GAME.</p>
                        </li>
                        <li>
                            エンタテイメントコンピューティング2020  最優秀論文賞
                            <p>川崎仁史, 脇坂崇平, 笠原俊一, 齊藤寛人, 原口純也, 稲見昌彦. けん玉できた！VR：数分間のVRトレーニングによってけん玉の技の習得を支援するシステム.</p>
                        </li>
                        <li>
                            第21回計測自動制御学会システムインテグレーション部門講演会 優秀講演賞
                            <p>岡本直樹, 廣瀬雅治, 脇坂 崇平, 齊藤寛人, 泉原厚史, 稲見昌彦. 重心移動を用いた手部身体の変形システムの検討.</p>
                        </li>

                    </ul>
                </div>
            </section>
            <hr class="m-0" />
            
            <!-- contact-->
            <section class="resume-section" id="contact">
                <div class="resume-section-content">
                    <h3 class="mb-5">Contact</h3>
                    <ul>
                        <li>
                            hiroto.saito.613[at]gmail.com
                        </li>

                    </ul>
                    <a class="twitter-timeline" data-width="600" data-height="450" data-theme="light" href="https://twitter.com/SaitoHiroto?ref_src=twsrc%5Etfw">Tweets by SaitoHiroto</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
                    <footer>
                        <p> © 2021. Hiroto Saito All Rights Reserved.</p>
                    </footer> 
                </div>
                
            </section>
                    
            <hr class="m-0" />
        </div>
       
    </body>
</html>

